# ============================================================================
# Firecrawl Monorepo Environment Variables
# ============================================================================
#
# SINGLE SOURCE OF TRUTH: This file contains ALL environment variables for
# ALL services in the monorepo (Firecrawl API, MCP Server, Webhook Bridge).
#
# When deploying via docker-compose, this .env file is automatically loaded
# by all services via the common service anchor.
#
# Individual app .env.example files (apps/mcp/.env.example, etc.) are ONLY
# for standalone deployments and should NOT be used in the monorepo.
#
# Services included:
# - Firecrawl API (Node.js)
# - MCP Server (Node.js)
# - Webhook Bridge (Python)
# - Web Interface (Next.js)
# - Shared Infrastructure (PostgreSQL, Redis, Playwright)
#
# Copy this file to .env and update with your values.
# ============================================================================

# -----------------
# Core Firecrawl API
# -----------------
HOST=0.0.0.0
FIRECRAWL_PORT=50102
FIRECRAWL_INTERNAL_PORT=3002
FIRECRAWL_API_URL=http://localhost:50102
FIRECRAWL_API_KEY=your-api-key-here

# Worker Configuration
WORKER_PORT=50103
EXTRACT_WORKER_PORT=50106
NUM_WORKERS_PER_QUEUE=4  # Workers per queue (reduce to prevent PDF loop overload)
NUQ_WORKER_COUNT=4        # Match NUM_WORKERS_PER_QUEUE (actual variable Firecrawl reads)
WORKER_CONCURRENCY=2      # Jobs per worker (reduce to prevent PDF loop overload)
SCRAPE_CONCURRENCY=4      # Concurrent scrapes (reduce to prevent PDF loop overload)
RETRY_DELAY=3000          # Delay between retries (milliseconds)
MAX_RETRIES=1             # Max retry attempts (prevents infinite retry loops)

# -----------------
# PR #2381: Scrape Reliability & Cancellation
# -----------------
# Retry limits prevent infinite loops when blocked by anti-bot systems
SCRAPE_MAX_ATTEMPTS=6                   # Global cap across all error types
SCRAPE_MAX_FEATURE_TOGGLES=3            # Max feature additions during retry
SCRAPE_MAX_FEATURE_REMOVALS=3           # Max feature removals during retry
SCRAPE_MAX_PDF_PREFETCHES=2             # Max PDF antibot retries
SCRAPE_MAX_DOCUMENT_PREFETCHES=2        # Max document antibot retries

# Client disconnect cancellation
SCRAPE_CANCELLATION_POLL_INTERVAL_MS=1000  # Worker polling frequency for cancellation

# NuQ finalization retry
NUQ_FINALIZE_RETRY_ALERT_THRESHOLD=3    # Alert after N jobFinish/jobFail retries
NUQ_STALL_ALERT_THRESHOLD=10            # Alert after N stalled job reaps

# System Resources
MAX_CPU=1.0
MAX_RAM=1.0

# -----------------
# Shared Infrastructure
# -----------------

# PostgreSQL
POSTGRES_USER=firecrawl
POSTGRES_PASSWORD=your-secure-password
POSTGRES_DB=pulse_postgres
POSTGRES_PORT=50105
NUQ_DATABASE_URL=postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@pulse_postgres:5432/${POSTGRES_DB}

# Redis
REDIS_PORT=50104
REDIS_URL=redis://pulse_redis:6379
REDIS_RATE_LIMIT_URL=redis://pulse_redis:6379
BULL_AUTH_KEY=@

# Playwright
PLAYWRIGHT_PORT=50100
PLAYWRIGHT_MICROSERVICE_URL=http://pulse_playwright:3000/scrape
BLOCK_MEDIA=true

# -----------------
# MCP Server
# -----------------
# Port Configuration
MCP_PORT=50107
ALLOWED_HOSTS=localhost:50107

# Firecrawl Integration
MCP_FIRECRAWL_API_KEY=self-hosted-no-auth
MCP_FIRECRAWL_BASE_URL=http://firecrawl:3002

# LLM Provider Configuration (for extract feature)
MCP_LLM_PROVIDER=openai-compatible
MCP_LLM_API_BASE_URL=https://cli-api.tootie.tv/v1
MCP_LLM_MODEL=claude-haiku-4-5-20251001

# MCP Configuration
MCP_OPTIMIZE_FOR=cost
MCP_RESOURCE_STORAGE=memory
MCP_RESOURCE_FILESYSTEM_ROOT=/app/resources
MCP_RESOURCE_TTL=86400
DEBUG=false
FORCE_COLOR=1

# OAuth Configuration
MCP_ENABLE_OAUTH=false
MCP_GOOGLE_CLIENT_ID=
MCP_GOOGLE_CLIENT_SECRET=
MCP_GOOGLE_REDIRECT_URI=http://localhost:50107/auth/google/callback
MCP_GOOGLE_OAUTH_SCOPES=openid,email,profile
MCP_OAUTH_SESSION_SECRET=
MCP_OAUTH_TOKEN_KEY=
MCP_OAUTH_RESOURCE_IDENTIFIER=https://pulse-mcp.local
MCP_OAUTH_AUTHORIZATION_SERVER=https://accounts.google.com
MCP_OAUTH_TOKEN_TTL=3600
MCP_OAUTH_REFRESH_TTL=2592000
MCP_REDIS_URL=redis://pulse_redis:6379

# Map Tool Configuration
MCP_MAP_DEFAULT_COUNTRY=US
MCP_MAP_DEFAULT_LANGUAGES=en-US
MCP_MAP_MAX_RESULTS_PER_PAGE=200

# Optional: Remote Docker host for GPU acceleration
# Format: username@host.ip (e.g., user@192.168.1.100)
# Leave empty to skip remote Docker context creation
MCP_DOCKER_REMOTE_HOST=

# SSH Configuration for Remote Docker
# Mount your SSH directory to /mnt/ssh for remote Docker access
# Required files in the mounted directory:
#   - known_hosts: SSH known hosts file for the remote machine
#   - id_rsa or id_ed25519: Private key for SSH authentication
# Docker compose mount example:
#   volumes:
#     - /path/to/your/.ssh:/mnt/ssh:ro
# The entrypoint will copy known_hosts if present

# Webhook Service (for MCP query tool)
WEBHOOK_BASE_URL=http://pulse_webhook:52100
# ⚠️  SECURITY: Generate with: openssl rand -hex 32
# Minimum 32 characters required for production (WEBHOOK_TEST_MODE=false)
WEBHOOK_API_SECRET=REPLACE_WITH_OUTPUT_FROM_openssl_rand_hex_32

# MCP Webhook Configuration
# Comma-separated list of webhook events to send for crawl operations
# Options: page, started, completed, failed
# Default: page (reduces webhook traffic by 50% by filtering out started/completed events)
MCP_WEBHOOK_EVENTS=page

# Docker Logs Resource Configuration
# Path to docker-compose.yaml for log access (must be accessible from container)
MCP_DOCKER_COMPOSE_PATH=/compose/pulse/docker-compose.yaml
# Project name from docker-compose (defaults to directory name)
MCP_DOCKER_PROJECT_NAME=pulse
# External services on remote Docker contexts (JSON array, optional)
# Example: [{"name":"gpu_tei","description":"TEI on GPU host","context":"gpu-server"}]
# Requires: docker context create gpu-server tcp://gpu-host:2376 --docker tls-verify=true
MCP_DOCKER_EXTERNAL_SERVICES=

# -----------------
# Web Interface (Next.js)
# -----------------
NEXT_PUBLIC_API_URL=http://localhost:50102            # Firecrawl API base URL for Add Source / rescrapes
NEXT_PUBLIC_MCP_URL=http://localhost:50107            # MCP endpoint for chat + tool execution
NEXT_PUBLIC_WEBHOOK_URL=http://localhost:50108        # Webhook search API (hybrid search, stats)
NEXT_PUBLIC_GRAPH_URL=http://localhost:50210          # Neo4j Browser (optional Mind Map visualizer)
WEB_PORT=3000                                          # Web interface port

# -----------------
# Webhook Bridge (Search)
# -----------------
# Note: WEBHOOK_* variables are preferred. SEARCH_BRIDGE_* variables are still supported for backward compatibility.

# ====================
# SECURITY: Webhook Secrets
# ====================
# ⚠️  CRITICAL: Generate secure secrets for production!
#
# Quick setup (copy/paste):
#   export WEBHOOK_SECRET=$(openssl rand -hex 32)
#   export WEBHOOK_API_SECRET=$(openssl rand -hex 32)
#   export CHANGEDETECTION_WEBHOOK_SECRET=$(openssl rand -hex 32)
#   export WEBHOOK_CHANGEDETECTION_HMAC_SECRET=$CHANGEDETECTION_WEBHOOK_SECRET
#
# Or generate individually:
#   openssl rand -hex 32
#
# ⚠️  IMPORTANT: CHANGEDETECTION_WEBHOOK_SECRET and WEBHOOK_CHANGEDETECTION_HMAC_SECRET
#    MUST be identical for webhook signature verification to work!
#
# Development defaults provided below are UNSAFE for production.
# ====================

# Server Configuration
WEBHOOK_HOST=0.0.0.0
WEBHOOK_PORT=50108
# WEBHOOK_API_SECRET defined in MCP query tool section above
# ⚠️  SECURITY: Generate with: openssl rand -hex 32
# Minimum 32 characters required for production (WEBHOOK_TEST_MODE=false)
WEBHOOK_SECRET=REPLACE_WITH_OUTPUT_FROM_openssl_rand_hex_32

# CORS Configuration
# Comma-separated list of allowed origins for cross-origin requests
# Development: Include localhost origins for local development
# Production: Replace with your actual frontend domain(s)
# Security: Never use "*" wildcard - specify exact origins
WEBHOOK_CORS_ORIGINS=http://localhost:3000,http://localhost:50107

# Redis Configuration
WEBHOOK_REDIS_URL=redis://pulse_redis:6379

# PostgreSQL Configuration
WEBHOOK_DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@pulse_postgres:5432/${POSTGRES_DB}

# Vector Store (Qdrant)
WEBHOOK_QDRANT_URL=http://qdrant:6333
WEBHOOK_QDRANT_COLLECTION=pulse_docs
WEBHOOK_VECTOR_DIM=1024

# Embeddings (TEI)
WEBHOOK_TEI_URL=http://tei:80
WEBHOOK_EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B

# Chunking Configuration
WEBHOOK_MAX_CHUNK_TOKENS=256
WEBHOOK_CHUNK_OVERLAP_TOKENS=50

# Search Configuration
WEBHOOK_HYBRID_ALPHA=0.5
WEBHOOK_BM25_K1=1.5
WEBHOOK_BM25_B=0.75
WEBHOOK_RRF_K=60

# Logging
WEBHOOK_LOG_LEVEL=INFO

# Worker Configuration
# Enable background worker thread for processing indexing jobs
# IMPORTANT: Set to false when using pulse_webhook_worker container (recommended)
# The worker now runs as a separate container for proper signal handling and scalability
# Only set to true for standalone/development deployments without the worker container
WEBHOOK_ENABLE_WORKER=false

# Worker Batch Processing
WEBHOOK_WORKER_BATCH_SIZE=4  # Process 4 docs concurrently per worker (8 workers × 4 = 32 parallel docs)

# Job Configuration
WEBHOOK_INDEXING_JOB_TIMEOUT=10m                # RQ job timeout (e.g., 10m, 1h, 600s)

# -----------------
# Search Index Integration
# -----------------
# Enable automatic indexing of crawled pages to webhook bridge
ENABLE_SEARCH_INDEX=true

# Search service endpoint (external URL for API access)
SEARCH_SERVICE_URL=http://localhost:50108

# API secret for search endpoints (not used for webhooks)
SEARCH_SERVICE_API_SECRET=your-webhook-api-secret

# Sampling rate for indexing (0.0-1.0, where 1.0 = 100% of pages)
SEARCH_INDEX_SAMPLE_RATE=1.0

# Webhook Configuration
# IMPORTANT: Use internal Docker network URL for webhook delivery
# Format: http://<container-name>:<port>/api/webhook/firecrawl
# ❌ DON'T: https://external-domain.com/... (causes 502 errors)
# ✅ DO: http://pulse_webhook:52100/api/webhook/firecrawl
SELF_HOSTED_WEBHOOK_URL=http://pulse_webhook:52100/api/webhook/firecrawl

# Allow webhooks to internal Docker network addresses (bypasses SSRF protection)
# Required for internal Docker communication, safe for trusted networks
ALLOW_LOCAL_WEBHOOKS=true

# -----------------
# Change Detection Service
# -----------------
CHANGEDETECTION_PORT=50109
CHANGEDETECTION_BASE_URL=http://localhost:50109
CHANGEDETECTION_INTERNAL_URL=http://pulse_change-detection:5000
CHANGEDETECTION_FETCH_WORKERS=10
CHANGEDETECTION_MINIMUM_SECONDS_RECHECK_TIME=60

# Webhook secret for changedetection.io -> webhook bridge (generate with: openssl rand -hex 32)
# ⚠️  Development default below is UNSAFE for production - generate a new secret!
CHANGEDETECTION_WEBHOOK_SECRET=<64-char-hex>
# changedetection.io API key (get from Settings > API in changedetection.io UI)
CHANGEDETECTION_API_KEY=<changedetection-api-key>
WEBHOOK_CHANGEDETECTION_API_KEY=<changedetection-api-key>

# Webhook Bridge - changedetection integration
# ⚠️  CRITICAL: Must match CHANGEDETECTION_WEBHOOK_SECRET for signature verification!
WEBHOOK_CHANGEDETECTION_HMAC_SECRET=<64-char-hex>
WEBHOOK_FIRECRAWL_API_URL=<firecrawl-api-url>
WEBHOOK_FIRECRAWL_API_KEY=<firecrawl-api-key>

# Webhook Bridge - automatic watch creation
# Configure automatic creation of changedetection.io watches for scraped URLs
WEBHOOK_CHANGEDETECTION_API_URL=<changedetection-api-url>
WEBHOOK_CHANGEDETECTION_CHECK_INTERVAL=3600
WEBHOOK_CHANGEDETECTION_ENABLE_AUTO_WATCH=true

# -----------------
# Search Provider
# -----------------
SEARXNG_ENDPOINT=https://s.tootie.tv
SEARXNG_ENGINES=google,bing,duckduckgo,startpage,yandex
SEARXNG_CATEGORIES=general,images,videos,news,map,music,science

# -----------------
# External Services (GPU Machine)
# -----------------
# These services run on a separate machine with GPU support.
# Deploy using: pnpm services:external:up
# When deploying externally, uncomment and update these URLs to point to your GPU host:
# WEBHOOK_TEI_URL=http://gpu-machine-ip:50200
# WEBHOOK_QDRANT_URL=http://gpu-machine-ip:50201
# And comment out the localhost defaults above in the Webhook Bridge section.

# Host data root for GPU-side services
APPDATA_BASE=/mnt/cache/appdata

# Text Embeddings Inference (GPU machine)
TEI_HTTP_PORT=52000
TEI_EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
TEI_MAX_CONCURRENT_REQUESTS=80
TEI_MAX_BATCH_TOKENS=163840
TEI_MAX_BATCH_REQUESTS=80
TEI_MAX_CLIENT_BATCH_SIZE=128
TEI_POOLING=last-token
TEI_TOKENIZATION_WORKERS=8
NVIDIA_VISIBLE_DEVICES=0
CUDA_VISIBLE_DEVICES=0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
OMP_NUM_THREADS=8
MKL_NUM_THREADS=8
TOKENIZERS_PARALLELISM=true
CUDA_CACHE_DISABLE=0
RUST_LOG=text_embeddings_router=info
HF_HUB_CACHE=/data/cache
HF_HUB_ENABLE_HF_TRANSFER=1

# Qdrant Vector Database (GPU machine)
QDRANT_HTTP_PORT=52001
QDRANT_GRPC_PORT=52002
QDRANT__LOG_LEVEL=INFO
QDRANT__SERVICE__HTTP_PORT=6333
QDRANT__SERVICE__GRPC_PORT=6334
QDRANT__GPU__INDEXING=1
QDRANT__STORAGE__ON_DISK_PAYLOAD=true
QDRANT__STORAGE__OPTIMIZERS__MEMMAP_THRESHOLD=20000
QDRANT__TELEMETRY_DISABLED=true

# Ollama
OLLAMA_PORT=52003
OLLAMA_HOST=0.0.0.0:11434
WEBHOOK_OLLAMA_URL=http://gpu-machine:52003

# Neo4j Configuration
GRAPH_DB_USERNAME=neo4j
GRAPH_DB_PASSWORD=firecrawl_graph_2025
GRAPH_DB_HTTP_PORT=50210
GRAPH_DB_BOLT_PORT=50211
NEO4J_AUTH=${GRAPH_DB_USERNAME}/${GRAPH_DB_PASSWORD}
NEO4J_PLUGINS=["apoc", "graph-data-science"]

# Change-Detection.io Config
BASE_URL=http://localhost:50109
FETCH_WORKERS=10
MINIMUM_SECONDS_RECHECK_TIME=60
LOGGER_LEVEL=INFO
HIDE_REFERER=true

# OpenAI API (or compatible)
OPENAI_API_KEY=dummy
OPENAI_BASE_URL=https://cli-api.tootie.tv/v1
MODEL_NAME=claude-haiku-4-5-20251001

# Anthropic API
ANTHROPIC_API_KEY=your-anthropic-key

# Hugging Face Hub access token
HF_TOKEN=hf_your-token-here

# -----------------
# Application Settings
# -----------------
USE_DB_AUTHENTICATION=false

# Logging levels: ERROR, WARN, INFO, DEBUG
# INFO = General information (recommended for production)
# DEBUG = Verbose debugging (use when troubleshooting webhooks)
# WARN = Show warnings and errors (cleaner logs, hides PDF engine fallback warnings)
LOGGING_LEVEL=WARN
